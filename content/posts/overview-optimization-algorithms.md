---
title: "Optimizations Algorithms in Machine Learning"
date: 2021-07-11
draft: false
tags: ["Machine Learning", "Deep Learning", "Mathematics"]
categories: ["Mathematics", "Optimizations"]
---


### What is Gradient Descent?
Gradient descent is a first-order iterative optimization algorithm in which search direction is guided by the gradient information of the objective function or to be precise in the opposite direction of the gradient of the objective function. Hence, the name  **Gradient Descent**

$$\theta_{t+1} = \theta_t -  \eta \cdot \nabla_\theta L(\theta) $$

where $\nabla_\theta L(\theta)$  is the gradient with respect to $\theta$ .

<!-- ![Gradient_Descent](/posts/overview-optimization-algorithms/gradient-descent.webm) -->

Gradient Descent [1] - We randomly initiate parameters then these parameters move through these terrains, the valleys to reach their optimum value by moving in the direction opposite to its gradient.

```python
    #implementation of gradient descent
    def gradient_descent(params, x, y, epochs, lr=0.01):
    	for _ in range(epochs):
    		params_grad = grad(loss_fn, argnums = 0)(theta, x, y)
    		params = params - lr * params_grad
        return params
```


But why move in the direction opposite to gradient direction? Why gradient in the opposite direction is optimum?
    The answer comes from Taylor's Series, ignoring higher order terms

$$L\left( \theta +\eta \Delta \theta \right) =L\left( \theta \right) +\eta \Delta \theta ^{T}\cdot \nabla L\left( \theta \right) +\dfrac{n^{2}}{2!}\ast \triangle \theta ^{T}\nabla ^{2}L\left( \theta \right) \Delta \theta +\ldots$$

We want to estimate the loss function at a new value of parameters such that there should be a decrease in the loss function.

$$L\left( \theta +\eta \Delta \theta \right) -L\left( \theta \right) <0$$

Above equation also means that $\Delta \theta ^{T}\cdot L\left( \theta \right) <0$ and range of this $[-1,1]$ as it is a inner product of two vectors.

$$ -1 \leq cos \beta = \frac{\Delta \theta^T \cdot \nabla L(\theta)}{\|\theta^T\| \cdot \|\nabla L(\theta)\|} \leq 1$$

$$-\|\theta^T\| \cdot \|\nabla L(\theta)\| \leq cos \beta \|\theta^T\| \cdot \|\nabla L(\theta)\|= \Delta \theta^T \cdot \nabla L(\theta) \leq \|\theta^T\| \cdot \|\nabla L(\theta)\|$$

Therefore, $L\left( \theta +\eta \Delta \theta \right) -L\left( \theta \right) = cos \beta \|\theta^T\|$ is most negative when  $cos \beta$  is $-1$ i.e. when $\beta$ is $180^\degree$.

- In the context of machine learning, Gradient descent is an optimization algorithm to minimize an objective function i.e. error function or cost function $L(θ)$ that is parameterized by a model’s parameters $θ ∈ R^d$ by updating the parameters in the opposite direction of the gradient of the objective function $∇_θL(θ)$ w.r.t. to the parameters.
- In machine learning, we have assumed that the objective function is smooth and continuously differentiable. So we need to move in direction of the slope of the continuous surface generated by our objective function.  Gradient descent is a first-order optimization algorithm.
- To reach a global or local minimum, we have to decide a step size (how much to move in the opposite to gradient direction) that is dependent on the learning rate $\eta$.

#### Why Gradient Descent is Difficult?
  - Choosing a proper Learning Rate?
    - Selecting an optimum learning rate can be very difficult choice to make.
    - A very small learning rate will lead to slow convergence as it will require lot more iterations to reach its minimum.
    - And choosing a very large learning rate can make convergence very difficult as error function or loss function may oscillates around the minima. Also, there is always as possibility of divergence.
  - Losing generalization for Dataset while Scheduler
    - Learning rate schedules try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics
    - Difficulty in Updating Sparse Dataset
    - Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.
  - Get trapped in saddle points
    - Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima
    - Dauphin et al. [5] argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions
#### Variants of Gradient Descent

- ##### Batch gradient descent

Vanilla gradient descent or sometimes called batch gradient descent works on the complete training dataset and hence, updates the parameter after calculating the mean of gradients for all the training data points with respect to $\theta_i$.

$$\theta=\theta-\eta \cdot \nabla_{\theta} L\left(\theta ; x^{(i: i+n)} ; y^{(i: i+n)}\right)$$

We need to traverse the whole dataset to perform one update, which makes batch gradient descent computationally expensive.

Applying batch gradient descent when the dataset does not fit in memory, makes the algorithm slow and intractable. Another issue with batch gradient descent is updating the model on the run as in the case of online learning is not possible.

- ##### Stochastic gradient descent

Stochastic gradient descent (SGD) differs from batch gradient descent that it updates the parameters for each training examples

$$\theta=\theta-\eta \cdot \nabla_{\theta} L\left(\theta ; x^{(i)} ; y^{(i)}\right)$$

![overview](/posts/overview-optimization-algorithms/untitled.png)

As we can see vanilla gradient descent performs unnecessary computations for large datasets, as it computes gradient for each parameters and updates through each examples. SGD relaxes this computations by performing one update at a time for a single example. Therefore, SGD are much faster than vanilla gradient descent and can be used in online learning.

Due to high variance in updates in SGD leads to high frequency oscillations in the objective functions.   Vanilla gradient descent have  guaranteed to get to the minimum  of the basin of local minima or global minimum for any non-convex or convex surface. While SGD fluctuating behaviour can leads to better local minima but it also complicates the convergence as SGD keeps overshooting. However, if we keep a proper annealing rate SGD will converge like vanilla gradient descent    

- ##### Mini-batch gradient descent

Mini-batch takes the advantages of both the worlds of Vanilla gradient descent and SGD as it performs an update  based on the computations of very mini-batch of $b$.
$$\theta=\theta-\eta \cdot \nabla_{\theta} L\left(\theta ; x^{(i: i+b)} ; y^{(i: i+b)}\right)$$

Mini-batch gradient descent reduces the fluctuations i.e. stable convergence.
Mini-batch gradient descent is typically the algorithm of choice when training a deep neural network.
Mini-batch and SGD terms are used interchangeably in deep learning architecture.
#### Accelerating Gradient descent optimization algorithms
##### Momentum

In Momentum gradient descent, idea is that if we moving repeatedly in the same direction then we should be moving in faster rate with big steps in that direction or this can be seen as moving down a spherical ball in parabolic bowl. To do so, we add a momentum component to original stochastic gradient descent.

$$\begin{aligned}v_{t} &=\gamma v_{t-1}+\eta \nabla_{\theta} J(\theta) \\\theta &=\theta-v_{t}\end{aligned}$$

```python
        def grad_descent_momentum(params, x, y, epochs, gamma=0.9, lr=0.01):
        	velocity  = jnp.zeros(len(theta))
        	for _ in range(epochs):
        		params_grad = grad(loss_fn, argnums=0)(theta, x, y)
        		velocity = gamma * velocity + lr * params_grad
        		params = params - velocity
        	return params
        	
```

When using momentum i.e. moving a ball down a hill. Ball accumulates momentum as it goes down the hill, it will not slow down at the bottom of the hill. To reduce its momentum, we add damper into our equation which reduce the momentum as we go bottom of the hill. This results in faster convergence and reduced oscillations.

- ##### Nesterov accelerated gradient

In the regions that have gentle slopes, momentum  based gradient descent takes very large steps due to momentum it carries. And we want to have a higher steps  when slopes  are steeper and smaller updates when slope is gentle.

To get this, we provide momentum term some kind of prescience i.e. we want to compute the gradient based on the future update or look ahead position $\gamma v_{t-1}$. And the nudge the parameters based on this.

![An%20overview%20of%20gradient%20descent%20optimization%20algor%20672a924dd3dd4bb1b2bd51376d1d3f16/Untitled%201.png](An%20overview%20of%20gradient%20descent%20optimization%20algor%20672a924dd3dd4bb1b2bd51376d1d3f16/Untitled%201.png)

In Nesterov momentum,  instead of calculating gradient at the current position (red circle), we calculate gradient at "look-ahead" position [2].

Image Credit: 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-3/)

$$\begin{aligned}
        v_{t} &=\gamma v_{t-1}+\eta \nabla_{\theta} L\left(\theta-\gamma v_{t-1}\right)
        \theta_{t + 1} &=\theta_{t}-v_{t}
\end{aligned}$$

```python
        def grad_descent_momentum(params, x, y, epochs, gamma=0.9, lr=0.01):
        	velocity  = jnp.zeros(len(theta))
        	for _ in range(epochs):
        		params_grad = grad(loss_fn, argnums=0)((theta - gamma * velocity) , x, y)
        		velocity = gamma * velocity + lr * params_grad
        		params = params - velocity
        	return params
        	
 ```

##### AdaGrad

Adagrad is an optimization algorithm based on adaptive  learning rate. When we have sparse data i.e. most of the values are $0$. Their gradients will be $0$ most of the time. To mitigate this problem, decay  the learning rate for each parameters in proportional to its update history i.e. is to provide larger updates for infrequent and smaller updates for frequent parameters.

$g_{t,i}$ be the gradient of the objective function with respect to parameters $\theta_i$ at timestep $t$.

$$g_{t,i} = \nabla_{\theta_{t}} L(\theta_{t,i}) $$

Then the SGD update for every parameter $\theta_i$  for each time step $t$:

$$\theta_{t+1, i}=\theta_{t, i}-\eta \cdot g_{t, i}$$

In its parameter update rule, Adagrad modifies the learning rate $\eta$ based on cumulative sum of past gradients at time step $t.$ Here, $\varepsilon$ is a smoothing term to avoid division by the $0.$

$$\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i}$$

```python
        def ada_grad(params, x, y, epochs, lr=0.01, epsilon=1e-8):
            sq_grad = jnp.zeros(len(params))
            for _ in range(epochs):
                params_grad = grad(loss_fn, argnums=0)(params, x, y)
                sq_grad += params_grad ** 2
                params = params - (lr * params_grad) / (jnp.sqrt(sq_grad + epsilon))
            return params
```

Adagrad updates a different learning rate for each parameters $\theta_{i}$ for each time step $t$. Therefore, we need not to manually tune the learning rate $\eta$.

However, Adagard accumulate the squared gradients in the denominator for each time step, with every step learning rate $\eta$ get diminished by and get sufficiently small that it stops learning. 

  ##### Adadelta

One of the issue with Adagrad is that its monotonically decreasing learning rate that reaches a point where it stops learning and adagrad promises to reduce this aggressive decrease in learning rate.

To mitigate this, one ways is to restricts with some fixed size window $w$ of accumulated past gradients. Instead of storing inefficiently $w$ squared gradients, sum of gradients are computed as decayed average of all past squared gradients. Moving average of $E[g^2]_t$ is defined similar to momentum term.

$$E\left[ g^{2}\right] _{t}=\gamma E\left[ g^{2}\right] {t-1}+\left( 1-\gamma\right) g{t}^{2}$$

Typically the value of $\gamma$ is  around $0.9$. 

Similarly, decayed squared parameters updates

$$E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) \Delta \theta^{2}_{t}$$

Update rule for Adadelta

$$\theta_{t+1} = \theta - \frac{RMS[\Delta \theta]_{t-1}}{RMS[g_{t}]} g_t$$

where 

$$RMS[g_{t}] = \sqrt{[g^{2}]_t + \varepsilon}$$

$$RMS[\Delta \theta]_{t-1} = \sqrt{E[\Delta\theta^2]_t + \varepsilon
        }$$

$$\Delta \theta {t}=-\dfrac{\eta }{\sqrt{E\left[ g^{2}\right]_ t{+\;\varepsilon }}}g_{t}$$

        ```python
        def ada_delta(params, x, y, epochs, gamma_1= 0.9, gamma_2=0.9, epsilon=1e-8):
            sq_grad = jnp.zeros(len(params)) # Sum of Squared Gradients
            sq_up = jnp.zeros(len(params)) # Sum of Squared Updates
            for _ in range(epochs):
                params_grad = grad(loss_fn, argnums=0)(params, x, y)
                sq_grad += gamma_1 * sq_grad + (1 - gamma_1) * (params_grad ** 2)
                params_update = - ((jnp.sqrt(sq_up) + epsilon) / (jnp.sqrt(sq_grad) + epsilon)) * params_grad
                sq_up += gamma_2 * sq_up + (1 - gamma_2) * (params_update ** 2)
                params = params + params_update
            return params
        ```

        With Adadelta, we do not need separately need to work on learning rate as learning rate need to eliminated.

##### RMSprop

RMSprop is proposed by Geoff Hinton around same time as Adadelta. In fact, RMSprop is identical to the first parameter update to Adadelta 

$$\begin{aligned}E\left[g^{2}\right]_{t} &=0.9 E\left[g^{2}\right]_{t-1}+0.1 g_{t}^{2} \\\theta_{t+1} &=\theta_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t}\end{aligned}$$

```python
        def RMS_prop(params, x, y, epochs, lr=0.01, gamma=0.9, epsilon=1e-8):
            sq_grad = jnp.zeros(len(params))
            for _ in range(epochs):
                params_grad = grad(loss_fn, argnums=0)(params, x, y)
                sq_grad += gamma * sq_grad + (1 - gamma) * (params_grad ** 2)
                params = params - (lr * params_grad) / (jnp.sqrt(sq_grad) +  epsilon)
            return params
```

##### Adaptive Moment Estimation (Adam)

ADAM is combination of RMSprop and Momentum as it combines the exponentially decaying average of past squared gradients $v_t$ and decaying average of gradients as in case of momentum.

ADAM stores exponentially decaying average of past squared gradients $v_t$ as well as exponentially decaying average of past gradients $m_t$ which is kind of similar to momentum:

$$\begin{aligned}m_{t}=\beta_ {1}m_{t-1}+\left( 1-\beta_ {1}\right) g_{t}\\
        v_{t}=\beta_ {2}v{t-1}+\left( 1-\beta_ {2}\right) g_{t}^{2}\end{aligned}$$

$m_t$ and $v_t$ are estimates are based on first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively

As $m_t$ and $v_t$ are initialized  with vectors of  $0's$, ADAM observed to be biased towards zero, especially during the initial time steps as usually decay rates are small (i.e. $β_1$ and $β_2$ are close to 1)

To counteract these biases by computing bias-corrected first and second moment estimates:

$$\begin{aligned}\widehat{m}_{t}=\dfrac{m_{t}}{1-\beta_ {1}^t}\\
        \widehat{v}_{t}=\dfrac{v_{t}}{1-\beta _{2}^{t}}\end{aligned}$$

Therefore, the update rule for ADAM update rules are:

$$\theta_ {t+1}=\theta_ {t}-\dfrac{\eta}{\sqrt{\widehat{v}_{t}+\epsilon}}\widehat{m}_{t}$$

The authors propose default values of $0.9$ for $β_1$, $0.999$ for $\beta_2$, and $1e-8$ for $\varepsilon$.

```python
        def adam(params, x, y, epochs, beta_1= 0.9, beta_2=0.999, lr=0.1, epsilon=1e-8):
            m_t = jnp.zeros(len(params))
            v_t = jnp.zeros(len(params))
            for i in range(1, epochs+1):
                params_grad = grad(loss_fn, argnums=0)(params, x, y)
                mean = beta_1 * mean + ((1 - beta_1) * mean)
                variance = beta_2 * variance + ((1 - beta_2) * (params_grad ** 2))
                mean_hat = mean / (1 - (beta_1**i))
                variance_hat = mean / (1 - (beta_2 ** i))
                params = params - (lr * mean_hat) / (jnp.sqrt(variance_hat) + epsilon)

            return params
```

##### AdaMax

AdaMax is the generalization of ADAM to $L^p$ norm.  As we see that $v_t$ factor in the ADAM update rule scales the gradient inversely proportionally to the $L^2$  norm of the past gradients (via the $v_{t−1}$ term) and current gradient $|gt|^2$:

$$v_{t}=\beta_ {2}v_{t-1}+\left( 1-\beta_ {2}\right) \left| g_{t}\right| ^{2}$$

To generalize this update to the  $L^p$ norm, but with large $p$ values leads to unstable convergence. Hence, mostly in practice $L^1$ and $L^2$ being used.

$$v_{t}=\beta_ {2}^{p}v_{t-1}+\left( 1-\beta_ {2}^{p}\right) \left| g{t}\right| ^{p}$$

However,  $L^\infty$ also exhibits stable behavior.

We use $u_t$ to denote the $L^\infty$ norm-constrained $v_t$:

$$\begin{aligned}u_{t}&=\beta_ {2}^{\infty }v_{t-1}+\left( 1-\beta _{2}^{\infty }\right)\mid g _{t}\mid^{\infty} 
        \\&=\max \left( \beta_ {2}\cdot v_{t-1}\ \mid g _{t}\mid\right) \end{aligned}$$

Therefore, AdaMax update rule:

$$\theta_ {t+1}=\theta_ {t}-\dfrac{\eta}{u_{t}}\hat m_{t}$$

```python
        def adammax(params, x, y, epochs, beta_1= 0.9, beta_2=0.999, lr=0.1, epsilon=1e-8):
            m_t = jnp.zeros(len(params))
            u_t = jnp.zeros(len(params))
            for i in range(1, epochs+1):
                params_grad = grad(loss_fn, argnums=0)(params, x, y)
                m_t = beta_1 * m_t + ((1 - beta_1) * m_t)
                u_t = jnp.maximum(beta_2 * v_t, abs(params_grad))
                m_t_hat = m_t / (1 - (beta_1**i))
                params = params - (lr * m_t_hat) / (u_t)

            return params
```

##### Nadam

NADAM is a combination of ADAM and NAG as it integrates ADAM step with gradients at a look ahead time step except it updates the look ahead momentum vector instead look ahead gradient vector. Therefore, applying look ahead momentum vector directly to current parameters

$$\begin{aligned}g_{t} &=\nabla_{\theta_{t}} J\left(\theta_{t}\right) \\m_{t} &=\gamma m_{t-1}+\eta g_{t} \\\theta_{t+1} &=\theta_{t}-\left(\gamma m_{t}+\eta g_{t}\right)\end{aligned}$$

In order to add ADAM to Nesterov momentum, we replace bias corrected  $\hat m_t$ in terms of $m_t$. However, we do not need to modify $\hat v_t$.

$$\begin{aligned}m_{t} &= \beta_{1}m_{t-1} + (1 - \beta_1)g_t \\\hat m_{t} &=\frac{m_t}{1 - \beta_{1}^{t}} \\\theta_{t+1} &=\theta_{t}-\frac{\eta}{\sqrt{\hat v_t} + \epsilon}\hat m_t\end{aligned}$$

Replacing and expanding the Nesterov Momentum with above equations which gives us:

$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat v_t} + \epsilon}(\frac{\beta_1m_{t-1}}{1-\beta_{1}^{t}} + \frac{(1 - \beta_1)g_t}{1 - \beta_{1}^{t}})$$

Putting bias-corrected estimate of the momentum vector of the previous time step into the above equation:

$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat v_t} + \epsilon}(\beta_1 \hat m_{t-1} + \frac{(1 - \beta_1)g_t}{1 - \beta_{1}^{t}})$$

simply replacing this bias-corrected estimate of the momentum vector of the previous time step $\hat m_{t-1}$with the bias-corrected estimate of the current momentum vector $\hat m_t$, which gives us the Nadam update rule:

$$\theta _{t+1}=\theta_ {t}-\dfrac{\eta }{\sqrt{\widehat{v}_{t}}+\varepsilon}\left( \beta_ {1}\widehat m_{t}+\dfrac{\left( 1-\beta_ {1}\right) g_{t}}{1-\beta _{1}t}\right)$$

```python
        def nadam(params, x, y, epochs, beta_1= 0.9, beta_2=0.999, lr=0.1, epsilon=1e-8):
            m_t = jnp.zeros(len(params))
            v_t = jnp.zeros(len(params))
            for i in range(1, epochs+1):
                params_grad = grad(loss_fn, argnums=0)(params, x, y)
                m_t = beta_1 * m_t + ((1 - beta_1) * m_t)
                v_t = beta_2 * v_t + ((1 - beta_2) * (params_grad ** 2))
                m_t_hat = m_t / (1 - (beta_1**i))
                v_t_hat = m_t / (1 - (beta_2 ** i))
                params = params - (lr * (beta_1*m_t_hat + ((1 - beta_1) * params_grad)/(1 - (beta_1)**i))) / (jnp.sqrt(v_t_hat) + epsilon)

            return params
```

##### Hypergradient

Above mentioned gradient descent methods are very sensitive to the learning rate or adapt the learning rate during the execution of gradient descent. Learning rate dictates the sensitivity of gradient descent as too high or low learning rate drastically affects the performance.

A hypergradient is a gradient with respect to a hyperparameter. Hyperparameter algorithms decrease the sensitivity of the hyperparameter as it allows it to adapt more quickly.

Hypergradient is all about applying gradient descent for learning rate $\eta.$ And this requires finding a partial derivative of the objective function with respect to the learning rate.

$$\begin{aligned}\dfrac{\partial L\left( \theta_ {t+1}\right) }{\partial \eta }&=\left( g_{t+1}\right) ^{T}\cdot \dfrac{\partial }{\partial \eta }\left( \theta_ {t}-\eta g_{t-1}\right) \\
        &=\left( g_{t+1}\right) ^{T}\cdot \left( -g_{t-1}\right) \end{aligned}$$

Computing the hypergradient thus requires keeping track of the last gradient.

$$\begin{aligned}\theta _{t+1}&=\theta _{t}-\mu\nabla _{\eta }L\left( \theta_ {t}\right) \\
        &=\theta {t}+\mu \left( g_{t}\right) ^{T}\cdot \left( g_{t-1}\right) 
    \end{aligned}$$

where $μ$ is the hypergradient learning rate.

```python
    def hypergrad(params, x, y, epochs, hyper_lr, lr=0.01):
        prev_grad = jnp.zeros(len(params))
        for _ in range(epochs):
            params_grad = grad(loss_fn, argnums=0)(params, x, y)
            lr = lr + hyper_lr * (params_grad * prev_grad)
            prev_grad, lr = params_grad, lr
            params = params - lr * params_grad
        return params--
```